{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51b91f74-7580-4a90-a2de-76706d67c289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CelebA images extracted to dataset/real\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# # Create the destination folder\n",
    "os.makedirs(\"dataset/real\", exist_ok=True)\n",
    "\n",
    "# # Unzip the CelebA zip file\n",
    "with zipfile.ZipFile(\"img_align_celebA.zip\", 'r') as zip_ref:\n",
    "     zip_ref.extractall(\"dataset/real\")\n",
    "\n",
    "print(\"CelebA images extracted to dataset/real\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d30f9b0-4098-4b43-9b10-68141f0f023a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFHQ images extracted to dataset/fake\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# # Create the destination folder for fake images\n",
    "os.makedirs(\"dataset/fake\", exist_ok=True)\n",
    "\n",
    "# # Unzip the SFHQ zip file\n",
    "with zipfile.ZipFile(\"images.zip\", 'r') as zip_ref:\n",
    "     zip_ref.extractall(\"dataset/fake\")\n",
    "\n",
    "print(\"SFHQ images extracted to dataset/fake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba6a9dfb-10b8-47b1-a0da-7f9d56da1d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1+cu101\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12e9fcf2-e920-4441-9ff8-3b0e1892f754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Check device whether it is working in gpu environment or not\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Dataset path\n",
    "dataset_path = './dataset'\n",
    "\n",
    "# Resizing and Normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10)\n",
    "])\n",
    "# Data Transformations\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "842bfdc6-49fb-4405-91c4-a2717d1fea86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./dataset\\\\fake\\\\SFHQ_pt3_00000001.jpg'\n",
      " './dataset\\\\fake\\\\SFHQ_pt3_00000002.jpg'\n",
      " './dataset\\\\fake\\\\SFHQ_pt3_00000003.jpg' ...\n",
      " './dataset\\\\real_and_fake\\\\SFHQ_pt3_00070540.jpg'\n",
      " './dataset\\\\real_and_fake\\\\SFHQ_pt3_00070541.jpg'\n",
      " './dataset\\\\real_and_fake\\\\SFHQ_pt3_00070542.jpg']\n",
      "[0 0 0 ... 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Gather image paths and labels manually\n",
    "class_names = sorted(os.listdir(dataset_path))  # Example: ['real', 'fake']\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for idx, class_name in enumerate(class_names):\n",
    "    class_dir = os.path.join(dataset_path, class_name)\n",
    "    for img_name in os.listdir(class_dir):\n",
    "        image_paths.append(os.path.join(class_dir, img_name))\n",
    "        labels.append(idx) #ASSIGN LABELS (Fake : 0 and Real:1)\n",
    "\n",
    "image_paths = np.array(image_paths)\n",
    "labels = np.array(labels)\n",
    "print(image_paths)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d62da53a-7a20-468f-9039-9b521bdc6436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./dataset\\\\real\\\\002897.jpg' './dataset\\\\fake\\\\SFHQ_pt3_00004966.jpg'\n",
      " './dataset\\\\fake\\\\SFHQ_pt3_00009424.jpg' ...\n",
      " './dataset\\\\fake\\\\SFHQ_pt3_00004543.jpg' './dataset\\\\real\\\\008976.jpg'\n",
      " './dataset\\\\real\\\\007205.jpg']\n",
      "[1 0 0 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Select only 15000 real and 15000 fake images\n",
    "real_indices = np.where(labels == 0)[0][:15000]\n",
    "fake_indices = np.where(labels == 1)[0][:15000]\n",
    "\n",
    "selected_indices = np.concatenate((real_indices, fake_indices))\n",
    "np.random.shuffle(selected_indices)\n",
    "\n",
    "selected_image_paths = image_paths[selected_indices]\n",
    "selected_labels = labels[selected_indices]\n",
    "print(selected_image_paths)\n",
    "print(selected_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a61cd0b2-1ef0-4384-a867-5948921d4eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split into training and validation sets (80-20 split)\n",
    "split = int(0.8 * len(selected_indices))\n",
    "train_image_paths = selected_image_paths[:split]\n",
    "train_labels = selected_labels[:split]\n",
    "val_image_paths = selected_image_paths[split:]\n",
    "val_labels = selected_labels[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6e604c0-fa73-4c92-b2d9-71c33b69200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Custom Dataset to resize only selected images\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a8f37ed-acc4-4dbd-8ed1-92ef9fea55d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomImageDataset(train_image_paths, train_labels, transform=train_transform)\n",
    "val_dataset = CustomImageDataset(val_image_paths, val_labels, transform=val_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68209f04-3e52-4029-bca1-7419bb6cac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fad73ed0-a523-429f-9ee8-3a52640f2a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Setup\n",
    "# model = models.squeezenet1_1(pretrained=True)\n",
    "# model.classifier[1] = nn.Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
    "# model.num_classes = 2\n",
    "# model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6de23aaa-5c5f-4298-af56-1d138fb8d8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.mobilenet_v2(pretrained=True)\n",
    "model.classifier[1] = nn.Linear(model.last_channel, 2)\n",
    "model.num_classes = 2\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19957921-b71f-4ee3-a332-bb4169043b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        pred = self.log_softmax(x)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b363be9-9a9e-4d44-a96b-41941b7c0060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = LabelSmoothingLoss(classes=2, smoothing=0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed84ea60-fe63-4d27-9509-3bd9fd6f894b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.long().to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = correct.double() / len(train_loader.dataset)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "        validate_model(model, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3e27f7d-fe81-49c0-ab07-d8f202129605",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation function\n",
    "def validate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.long().to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += torch.sum(preds == labels.data)\n",
    "\n",
    "    val_acc = correct.double() / len(val_loader.dataset)\n",
    "    print(f'Validation Accuracy: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45613f46-2308-48fe-9609-dba4b42ad3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 0.3527, Accuracy: 0.9886\n",
      "Validation Accuracy: 1.0000\n",
      "Epoch 2/15, Loss: 0.3334, Accuracy: 0.9992\n",
      "Validation Accuracy: 1.0000\n",
      "Epoch 3/15, Loss: 0.3294, Accuracy: 0.9998\n",
      "Validation Accuracy: 1.0000\n",
      "Epoch 4/15, Loss: 0.3279, Accuracy: 0.9999\n",
      "Validation Accuracy: 1.0000\n",
      "Epoch 5/15, Loss: 0.3270, Accuracy: 1.0000\n",
      "Validation Accuracy: 1.0000\n",
      "Epoch 6/15, Loss: 0.3268, Accuracy: 0.9997\n",
      "Validation Accuracy: 1.0000\n",
      "Epoch 7/15, Loss: 0.3264, Accuracy: 0.9999\n",
      "Validation Accuracy: 1.0000\n",
      "Epoch 8/15, Loss: 0.3262, Accuracy: 0.9998\n",
      "Validation Accuracy: 1.0000\n",
      "Epoch 9/15, Loss: 0.3263, Accuracy: 0.9996\n",
      "Validation Accuracy: 1.0000\n",
      "Epoch 10/15, Loss: 0.3258, Accuracy: 0.9999\n",
      "Validation Accuracy: 1.0000\n",
      "Epoch 11/15, Loss: 0.3259, Accuracy: 0.9998\n",
      "Validation Accuracy: 1.0000\n",
      "Epoch 12/15, Loss: 0.3255, Accuracy: 1.0000\n",
      "Validation Accuracy: 1.0000\n",
      "Epoch 13/15, Loss: 0.3255, Accuracy: 1.0000\n",
      "Validation Accuracy: 1.0000\n",
      "Epoch 14/15, Loss: 0.3255, Accuracy: 1.0000\n",
      "Validation Accuracy: 1.0000\n",
      "Epoch 15/15, Loss: 0.3257, Accuracy: 0.9999\n",
      "Validation Accuracy: 0.9998\n"
     ]
    }
   ],
   "source": [
    "# Start Training\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd92cb86-069a-4884-8243-9857e369c72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model parameters, model states\n",
    "torch.save(model.state_dict(), 'model_updated_3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "929ad32e-3dc2-4e01-b94d-33fcb0348a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully exported to model_updated_3.onnx with fixed batch size 1.\n"
     ]
    }
   ],
   "source": [
    "# New Cell to Export Model to ONNX\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Ensure the model architecture matches your trained model\n",
    "# This is crucial for successful ONNX export\n",
    "model = models.mobilenet_v2(pretrained=False) # No need for pretrained weights during export\n",
    "model.classifier[1] = nn.Linear(model.last_channel, 2) # Adjust final layer for 2 classes\n",
    "model.num_classes = 2 # Set num_classes for consistency\n",
    "\n",
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load('model_updated_3.pth'))\n",
    "model.eval() # Set the model to evaluation mode for inference\n",
    "\n",
    "# Define a dummy input tensor with a FIXED batch size of 1\n",
    "# This is the key change for the ONNX export\n",
    "# Shape: [batch_size, channels, height, width]\n",
    "dummy_input = torch.randn(1, 3, 128, 128) # 1 image, 3 color channels, 128x128 pixels\n",
    "\n",
    "# Define the ONNX export path\n",
    "onnx_model_path = \"model_updated_3.onnx\"\n",
    "\n",
    "# Export the model to ONNX\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    onnx_model_path,\n",
    "    export_params=True,        # Export trained parameter weights inside the model file\n",
    "    opset_version=11,          # The ONNX opset version to use (11 is common and widely supported)\n",
    "    do_constant_folding=True,  # Whether to execute constant folding for optimization\n",
    "    input_names=['input'],     # The name to assign to the input node in the ONNX graph\n",
    "    output_names=['output'],   # The name to assign to the output node in the ONNX graph\n",
    "    # NO dynamic_axes for the batch dimension (index 0)\n",
    "    # If you had dynamic_axes={'input': {0: 'batch_size'}}, remove it for fixed batch size.\n",
    ")\n",
    "\n",
    "print(f\"Model successfully exported to {onnx_model_path} with fixed batch size 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96e8ff9-141c-4065-b29d-480254086ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (StyleGAN2)",
   "language": "python",
   "name": "stylegan2-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
